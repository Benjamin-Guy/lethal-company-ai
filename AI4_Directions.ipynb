{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import os\n",
    "import cv2 \n",
    "import keyboard\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps, ImageGrab\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import io, color, measure, morphology\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import rotate\n",
    "import copy\n",
    "\n",
    "from gtts import gTTS\n",
    "import uuid\n",
    "from playsound import playsound\n",
    "import pyautogui\n",
    "import pydirectinput\n",
    "import vgamepad as vg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "I have made this code specific to run on windowed fullscreen 1920x1080 resolution in-game. I have hard-coded a lot of pixel values based on this assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item detection\n",
    "Scrap items appear to all appear at roughly the same shape, size, and colour. This suggests template matching as a solution. However, upon examining the training images, it was noted that each scrap item had the exact same colour albeit for a small difference observed in one case. This suggests that precise colour thresholding will be very effective at isolating the scrap items from the rest of the image. The steps for the item detection are as follows:\n",
    "\n",
    "- RGB colour threeshold applied to the image to identify scrap items.\n",
    "- Dilate segments of scrap items. This helps prevent map elements that sometimes break up the appearance of scrap as appearing as more than one scrap item when it is only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = cv2.imread(r'training\\9.png')[75:835, 395:1426]\n",
    "\n",
    "# Colour threshold for scrap items\n",
    "scrap_lower_bound = (1, 254, 172)\n",
    "scrap_upper_bound = (2, 255, 177)\n",
    "\n",
    "# Colour threshold the image\n",
    "color_mask = cv2.inRange(monitor, scrap_lower_bound, scrap_upper_bound)\n",
    "\n",
    "# Dilate segments\n",
    "kernel_size = (5, 5)\n",
    "kernel = np.ones(kernel_size, np.uint8)\n",
    "eroded_mask = cv2.erode(color_mask, kernel, iterations=1)\n",
    "item_mask = cv2.dilate(eroded_mask, kernel, iterations=4)\n",
    "\n",
    "cv2.imshow('Original Image', monitor)\n",
    "cv2.imshow('Binary Mask', item_mask)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enemy detection\n",
    "Similar to scrap items, enemies all have the same shape and colour, but have differences in size. There are also issues with other objects appearing the same colour as the enemies on the monitor. This necessitates a requirement for more sophisticated methods of detection. Circular Hough Transforms were tried, but too difficult to implement. A simple solution was found by using morphological opening to filter out non-circular shaped objects that proved better than the circular hough transform. This will be used to improve the enemy detection method.\n",
    "\n",
    "- RGB colour threshold applied to the image to filter out noise and any objects not red.\n",
    "- Morphological opening (erode, then dilate) to remove non-circular segments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = cv2.imread(r'training\\17.png')[75:835, 395:1426]\n",
    "# 5, 8\n",
    "\n",
    "# Colour threshold for enemies\n",
    "enemy_lower_bound = (1, 1, 85)\n",
    "enemy_upper_bound = (25, 25, 255)\n",
    "\n",
    "# Colour threshold the image\n",
    "color_mask = cv2.inRange(monitor, enemy_lower_bound, enemy_upper_bound)\n",
    "\n",
    "# Morphological Opening\n",
    "kernel_size = (4, 4)\n",
    "kernel = np.ones(kernel_size, np.uint8)\n",
    "dilated_mask = cv2.erode(color_mask, kernel, iterations=2)\n",
    "enemy_mask = cv2.dilate(dilated_mask, kernel, iterations=3)\n",
    "\n",
    "cv2.imshow('Original Image', monitor)\n",
    "cv2.imshow('Binary Mask', enemy_mask)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player Angle Detection\n",
    "\n",
    "Simplify the method used to detect what direction the player is looking in, and return one of four simple directions: (front, behind, left, right). There were somoe difficulties here as the colour thresholding is not as effective. However, the cases where this become an issue are quite limited. The process for detecting the angle the player is looking in is as follows:\n",
    "\n",
    "- Crop the image to just the player.\n",
    "- Blue colour thresholding on the image to isolate the player.\n",
    "- Average a centroid for the segmented areas and calculate the angle based on the centre of the image to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.14058270723698\n"
     ]
    }
   ],
   "source": [
    "def calculate_angle(binary_mask):\n",
    "    # Calculate the centroid of the white pixels in the binary mask\n",
    "    Y, X = np.where(binary_mask == 255)\n",
    "    centroid_x = np.mean(X)\n",
    "    centroid_y = np.mean(Y)\n",
    "\n",
    "    # Determine the center point of the image\n",
    "    image_center_x, image_center_y = binary_mask.shape[1] / 2, binary_mask.shape[0] / 2\n",
    "\n",
    "    # Calculate the angle to the centroid of the largest segment\n",
    "    delta_x = centroid_x - image_center_x\n",
    "    delta_y = image_center_y - centroid_y  # Image coordinates are top-left, so we invert the y-axis\n",
    "    angle_rad = math.atan2(delta_y, delta_x)  # Calculate angle in radians\n",
    "    # Adjust the angle so 0 degrees is 'up' and positive angles are clockwise\n",
    "    angle_deg = (90 - math.degrees(angle_rad)) % 360\n",
    "\n",
    "    return angle_deg\n",
    "\n",
    "monitor = cv2.imread(r'training\\16.png')[75:835, 395:1426]\n",
    "player = monitor[307:473, 453:619]\n",
    "\n",
    "# Colour thresholds for the player\n",
    "player_lower_bound = (90, 1, 1)\n",
    "player_upper_bound = (255, 200, 10)\n",
    "\n",
    "# Colour threshold the image\n",
    "color_mask = cv2.inRange(player, player_lower_bound, player_upper_bound)\n",
    "\n",
    "# Dilate to fill in any noise\n",
    "kernel_size = (5, 5)\n",
    "kernel = np.ones(kernel_size, np.uint8)\n",
    "player_mask = cv2.dilate(color_mask, kernel, iterations=3)\n",
    "\n",
    "angle = calculate_angle(player_mask)\n",
    "print(angle)\n",
    "\n",
    "cv2.imshow('Monitor', player_mask)\n",
    "cv2.imshow('Player', player)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting Functions\n",
    "Support functions such as converting a degree angle to a direction, getting centroids of detected objects (segments), and calculating relative angle of detected objects from player angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(image_mask):\n",
    "    contours, _ = cv2.findContours(image_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    centroids = []\n",
    "\n",
    "    # Loop through each contour\n",
    "    for cnt in contours:\n",
    "        # Calculate the moments of the contour\n",
    "        M = cv2.moments(cnt)\n",
    "        if M[\"m00\"] != 0:\n",
    "            # Calculate centroid coordinates\n",
    "            cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "            centroids.append((cX, cY))\n",
    "            # You can also draw the centroid on the image if you wish\n",
    "            cv2.circle(monitor, (cX, cY), 5, (255, 0, 0), -1)\n",
    "\n",
    "    return centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_direction(player_angle, object_centroid):\n",
    "    player_centroid = (536, 390)\n",
    "    \n",
    "    # Calculate angle from player to object\n",
    "    dy = object_centroid[1] - player_centroid[1]\n",
    "    dx = object_centroid[0] - player_centroid[0]\n",
    "    angle_to_object = math.atan2(dy, dx) * (180 / math.pi)\n",
    "    \n",
    "    # Adjust the game's coordinate system to a standard coordinate system with 0 degrees as up\n",
    "    angle_to_object = (angle_to_object + 90) % 360\n",
    "    \n",
    "    # Calculate relative angle to player's facing direction\n",
    "    relative_angle = (angle_to_object - player_angle) % 360\n",
    "    if relative_angle > 180:\n",
    "        relative_angle -= 360  # Adjust to have a range from -179 to 180\n",
    "    \n",
    "    # Determine direction based on the relative angle\n",
    "    if -45 < relative_angle <= 45:\n",
    "        return \"front\"\n",
    "    elif 45 < relative_angle <= 135:\n",
    "        return \"right\"\n",
    "    elif -135 <= relative_angle < -45:\n",
    "        return \"left\"\n",
    "    else:\n",
    "        return \"behind\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(binary_mask):\n",
    "    # Calculate the centroid of the white pixels in the binary mask for the player based on their own mask\n",
    "    Y, X = np.where(binary_mask == 255)\n",
    "    centroid_x = np.mean(X)\n",
    "    centroid_y = np.mean(Y)\n",
    "\n",
    "    # Determine the center point of the image\n",
    "    image_center_x, image_center_y = binary_mask.shape[1] / 2, binary_mask.shape[0] / 2\n",
    "\n",
    "    # Calculate the angle to the centroid of the largest segment\n",
    "    delta_x = centroid_x - image_center_x\n",
    "    delta_y = image_center_y - centroid_y  # Image coordinates are top-left, so we invert the y-axis\n",
    "    angle_rad = math.atan2(delta_y, delta_x)  # Calculate angle in radians\n",
    "    # Adjust the angle so 0 degrees is 'up' and positive angles are clockwise\n",
    "    angle_deg = (90 - math.degrees(angle_rad)) % 360\n",
    "\n",
    "    return angle_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text):\n",
    "    # Create a gTTS object\n",
    "    tts = gTTS(text=text, lang='en', tld='us')\n",
    "    filename = \"output_{}.mp3\".format(str(uuid.uuid1()))\n",
    "\n",
    "    # Save the speech to an MP3 file\n",
    "    tts.save(filename)\n",
    "\n",
    "    pyautogui.mouseDown()\n",
    "    # Play the audio file (this will play through the default audio device)\n",
    "    playsound(filename)\n",
    "    pyautogui.mouseUp()\n",
    "    \n",
    "    # Clean up\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_items(monitor):\n",
    "    # Colour threshold for scrap items\n",
    "    scrap_lower_bound = (1, 254, 172)\n",
    "    scrap_upper_bound = (2, 255, 177)\n",
    "\n",
    "    # Colour threshold the image\n",
    "    color_mask = cv2.inRange(monitor, scrap_lower_bound, scrap_upper_bound)\n",
    "\n",
    "    # Dilate segments\n",
    "    kernel_size = (5, 5)\n",
    "    kernel = np.ones(kernel_size, np.uint8)\n",
    "    eroded_mask = cv2.erode(color_mask, kernel, iterations=1)\n",
    "    item_mask = cv2.dilate(eroded_mask, kernel, iterations=4)\n",
    "\n",
    "    # Remove any items the player is holding or on top of\n",
    "    item_mask[307:473, 453:619] = 0\n",
    "\n",
    "    return get_centroids(item_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_enemies(monitor):\n",
    "    # Colour threshold for enemies\n",
    "    enemy_lower_bound = (1, 1, 85)\n",
    "    enemy_upper_bound = (25, 25, 255)\n",
    "\n",
    "    # Colour threshold the image\n",
    "    color_mask = cv2.inRange(monitor, enemy_lower_bound, enemy_upper_bound)\n",
    "\n",
    "    # Morphological Opening\n",
    "    kernel_size = (4, 4)\n",
    "    kernel = np.ones(kernel_size, np.uint8)\n",
    "    dilated_mask = cv2.erode(color_mask, kernel, iterations=2)\n",
    "    enemy_mask = cv2.dilate(dilated_mask, kernel, iterations=3)\n",
    "\n",
    "    return get_centroids(enemy_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player_angle(monitor):\n",
    "    player = monitor[307:473, 453:619]\n",
    "\n",
    "    # Colour thresholds for the player\n",
    "    player_lower_bound = (90, 1, 1)\n",
    "    player_upper_bound = (255, 200, 10)\n",
    "\n",
    "    # Colour threshold the image\n",
    "    color_mask = cv2.inRange(player, player_lower_bound, player_upper_bound)\n",
    "\n",
    "    # Dilate to fill in any noise\n",
    "    kernel_size = (5, 5)\n",
    "    kernel = np.ones(kernel_size, np.uint8)\n",
    "    player_mask = cv2.dilate(color_mask, kernel, iterations=3)\n",
    "\n",
    "    return calculate_angle(player_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detections(monitor):\n",
    "    # Make detections of items, enemies, and player angle\n",
    "    items = detect_items(monitor)\n",
    "    enemies = detect_enemies(monitor)\n",
    "    player_angle = get_player_angle(monitor)\n",
    "\n",
    "    # Initialize direction counters\n",
    "    directions = {\n",
    "        'enemy': {'front': 0, 'right': 0, 'behind': 0, 'left': 0},\n",
    "        'item': {'front': 0, 'right': 0, 'behind': 0, 'left': 0}\n",
    "    }\n",
    "\n",
    "    # Update counts for enemies\n",
    "    for centroid in enemies:\n",
    "        direction = get_relative_direction(player_angle, centroid)\n",
    "        directions['enemy'][direction] += 1\n",
    "\n",
    "    # Update counts for items\n",
    "    for centroid in items:\n",
    "        direction = get_relative_direction(player_angle, centroid)\n",
    "        directions['item'][direction] += 1\n",
    "\n",
    "    # Format the result string\n",
    "    results = []\n",
    "    for object_type in ['enemy', 'item']:\n",
    "        for direction in ['front', 'right', 'behind', 'left']:\n",
    "            count = directions[object_type][direction]\n",
    "            if count > 0:\n",
    "                results.append(f\"{count} {object_type} {direction}\")\n",
    "\n",
    "    # Join the results into a single string\n",
    "    detections = ', '.join(results)\n",
    "\n",
    "    return detections\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Now we want to get the centroids of all the items and the centroids of all the enemies and provide a detailed description of the count of enemies and items in each direction. This works acccurately in most cases but there are still a few times where objects are not stated in the correct direction to the player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 enemy front\n",
      "264.3057210023013\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "monitor = cv2.imread(r'training\\29.png')[75:835, 395:1426]\n",
    "print(detections(monitor))\n",
    "print(get_player_angle(monitor))\n",
    "print(detect_items(monitor))\n",
    "\n",
    "cv2.imshow('Test', monitor)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI4 Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\halvo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\halvo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\halvo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\halvo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     16\u001b[0m pyautogui\u001b[38;5;241m.\u001b[39mpress(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m image \u001b[38;5;241m=\u001b[39m ImageGrab\u001b[38;5;241m.\u001b[39mgrab(bbox\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m75\u001b[39m, \u001b[38;5;241m395\u001b[39m, \u001b[38;5;241m835\u001b[39m, \u001b[38;5;241m1426\u001b[39m))\n\u001b[0;32m     20\u001b[0m image_rgb \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time.sleep(10)\n",
    "import pyperclip\n",
    "\n",
    "while True:\n",
    "    time.sleep(1)\n",
    "    # Enter the terminal\n",
    "    pydirectinput.keyDown('e')\n",
    "    time.sleep(0.5)\n",
    "    pydirectinput.keyUp('e')\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Enter random nonsense to clear the screen (assuming we already have monitor up)\n",
    "    pyperclip.copy(\"aa\")\n",
    "    pyautogui.hotkey('ctrl', 'v')\n",
    "    time.sleep(0.1)\n",
    "    pyautogui.press('enter')\n",
    "    time.sleep(1)\n",
    "\n",
    "    image = ImageGrab.grab(bbox=(75, 395, 835, 1426))\n",
    "    image_rgb = image.convert('RGB')\n",
    "    numpy_image = np.array(image_rgb)\n",
    "    monitor = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
    "    speech = detections(monitor)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    # Exit the terminal\n",
    "    pydirectinput.keyDown('esc')\n",
    "    time.sleep(0.1)\n",
    "    pydirectinput.keyUp('esc')\n",
    "\n",
    "    # Use radio to tell any info\n",
    "    if speech:\n",
    "        pydirectinput.keyDown('q')\n",
    "        time.sleep(0.1)\n",
    "        pydirectinput.keyUp('q')\n",
    "        time.sleep(0.1)\n",
    "        pydirectinput.mouseDown(button='left')\n",
    "        time.sleep(0.2)\n",
    "        text_to_speech(speech)\n",
    "        time.sleep(0.1)\n",
    "        pydirectinput.mouseUp(button='left')\n",
    "        time.sleep(0.1)\n",
    "        pydirectinput.keyDown('q')\n",
    "        time.sleep(0.1)\n",
    "        pydirectinput.keyUp('q')\n",
    "        time.sleep(0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
